---
title       : Monte-Carlo Simulation
author      : Dhananjay Ghei
date        : 17th Sep 2017
job         : UMN
output      :
            html_document:
                toc: true
                toc_float: true
                mathjax: local
                self_contained: false
                keep_md: true
geometry    : margin=0.5in
theme       : cerulean
---


<style type="text/css">

body{ /* Normal  */
   font-size: 14px;
}

h1 { /* Header 1 */
 font-size: 22px;
 color: DarkBlue;
}
h2 { /* Header 2 */
   font-size: 19px;
   color: DarkBlue;
}
td{font-size: 11pt;
   padding:0px;
   cellpadding="0";
   cellspacing="0"
}
th {font-size: 11pt;
   height: 20px;
   font-weight: bold;
   text-align: center;
   background-color:
   #ccccff;
}
table{border-spacing:
	0px;
	border-collapse:
	collapse;
}

</style>
								
# Monte-Carlo Simulation

This article demonstrates a method to do Monte-Carlo simulations in R
for a two variable linear regression. The source code for this exercise
is available [online](https://www.github.com/dhananjayghei/econometrics) in the
Github repository.

## Setup
We will operate with the following model:

   $y_i = \beta_0 + x_i \beta_1 + \epsilon_i$
   
where, $y_i$ and $x_i$ are random variables. We observe $N$ samples
   of $i=1,2, \dots, n$ obersvations of $y$ and $x$.

We will estimate
   
   $y_i = \hat \beta_0 + x_i \hat \beta_1 + e_i$

```{r setOptions, message=FALSE, echo=FALSE, results='hide', warning=FALSE, include = FALSE}
source("monte_carlo.R")
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=6, echo=FALSE, warning=FALSE, message=FALSE)
```


## Drawing random samples
I write a function that draws $N$ random samples of size $n$ for the
variables $\epsilon$ and $x_i$. This function takes as arguments the
following:

1. ```muX``` - True mean of X
2. ```vX``` - True variance of X
3. ```muE``` - True mean of $\epsilon$
4. ```vE``` - True variance of $\epsilon$
5. ```r``` - Correlation between $\epsilon$ and $X$
6. ```n``` - Sample size
7. ```N``` - Number of samples


```{r drawFun, eval=FALSE, echo=TRUE, results='asis'}
draw <- function(muX, vX, muE, vE, r, n, N){
    set.seed(123)
    dat <- lapply(1:N, function(y){
        # Generating 2 random variables X and epsilon
        k <- mvrnorm(n=n, mu=c(muX, muE),
                     Sigma=matrix(c(vX, r, r, vE), nrow=2), empirical=FALSE)
        k <- data.frame(k)
        colnames(k) <- c("x", "eps")
        return(k)
    })
    return(dat)
}
```
## Generating $y$ (dependent variable)

Next, I define a function that designs a matrix which generates $y$
for given $X$ and $\epsilon$. It also adds a column of 1s to make
matrix multiplication easier. This function takes the following
arguments:

1. ```data``` - A sample of $X$ and $\epsilon$
2. ```b0``` - True value of $\beta_0$
3. ```b1``` - True value of $\beta_1$

```{r designFun, eval=FALSE, echo=TRUE, results='asis'}
design <- function(data, b0, b1){
    # Generating a column for intercecpt
    data[, "const"] <- rep(1, nrow(data))
    # Creating the true beta matrix
    betaT <- as.matrix(x=c(b0, b1), nrow=2, ncol=1)
    # Generating y = b0 + b1*x + epsilon
    data[, "y"] <- as.numeric(as.matrix(data[, c("const", "x")]) %*% betaT) + data[, "eps"]
    # Re-arranging the columns for easy readability
    data <- data[, c("y", "const", "x", "eps")]
    return(data)
}
```

## Putting it all together
The next step is to build a wrapper function that draws random
samples, designs a matrix for OLS, runs OLS on each sample and returns
output as the predicted values of coefficients ($\hat \beta_0$, $\hat
\beta_1$) and the variance-covariance matrix of $\hat \beta$. This
function takes as arguments values for both the ```draw``` function and the
```design``` function defined above.

```{r simulateFun, eval=FALSE, echo=TRUE, results='asis'}
simulate.OLS <- function(muX, vX, muE, vE, r, n, N, b0, b1){
    dat <- draw(muX=muX, vX=vX, muE=muE, vE=vE, r=r, n=n, N=N)
    dat <- lapply(dat, function(x) design(data=x, b0=b0, b1=b1))
    # Running OLS
    OLS <- lapply(dat, function(z){
        reg <- lm(y~x, data=z)
    })
    # Storing the estimated betas
    bhat <- data.frame(do.call(rbind, lapply(OLS, coef)))
    colnames(bhat) <- c("b0hat", "b1hat")
    rownames(bhat) <- NULL
    # Finding the x'x inverse
    xxInv <- lapply(dat, function(z){
            mat <- as.matrix(z[, c("const", "x")])
            Inv <- ginv(crossprod(mat))
            return(Inv)
    })
    # Finding the mean of inverse matrices
    xxInv.avg <- Reduce('+', xxInv)/n
    return(list(bEst=bhat, xxInv=xxInv.avg))
}

```

# Question 1
*First assume that the $\epsilon$ and the regressors are
uncorrelated. Assume that both $X$ and $\epsilon$ are drawn from a
normal distribution $\epsilon \sim N(0, \sigma_{\epsilon}^2), X \sim
N(\mu_x, \sigma_x^2)$. Start with a sample size ($n$) of
$n=100$ and define the number of samples to be simulated ($N$) to be
$N=100$ as well. You also need to define the true values of $\beta_0$
and $\beta_1$. Report the results that you fnid for the means and
variance-covariance matrix of $\hat \beta_0$ and $\hat
\beta_1$. Finally, plot the distribution of the estimated parameters
and the theoretical normal distribution that they are supposed to
follow.*

I use the following values for estimation:
$\beta_0=9$, $\beta_1=2$, $\mu_x=10$, $\mu_{\epsilon}=0$,
$\sigma_x^2=4$, $\sigma_{\epsilon}^2=1$, $r_{x\epsilon}=0$

I define the model with the above parameters as the *baseline*
model. As we move to the next questions, I compare the results of the
baseline model with the models estimated in the questions.

The implementation is given using the ```simulate.OLS``` function as
shown below:
```{r simulateQ1, echo=TRUE, eval=FALSE}
baseline <- simulate.OLS(muX=10, vX=4, muE=0, vE=1, r=0, n=100, N=100, b0=9, b1=2)
```
The table given below reports the parameters of interest from the
baseline model.

| Coefficients                       | Baseline Model |
|------------------------------------|---------------:|
| $E[\hat \beta_0|X]$                |          9.043 |
| $E[\hat \beta_1|X]$                |          1.997 |
| $v[\hat \beta_0|X]$                |          0.267 |
| $v[\hat \beta_1|X]$                |          0.003 |
| $cov(\hat \beta_0, \hat \beta_1|X)$|         -0.026 |



```{r beta0, echo = FALSE, results = 'asis', strip.white = TRUE}
plot(density(bhat.Baseline$b0hat),
     col="midnightblue", xlab="", ylab="Density", lwd=2, 
     main=expression(paste("Distribution of ", beta[0], sep="  ")))
lines(x=b0True[, 1], y=b0True[, 2], lwd=2, col="firebrick")
legend("topright", col=c("midnightblue", "firebrick"),
       lwd=c(2,2), bty="n", legend=c("Monte-carlo distribution",
                                     "True distribution"))
```

The figure above shows the distribution of $\hat \beta_0$ in the
baseline model compared to the *true* distribution of $\beta_0$. In
this case, the baseline model mimics the theoretical distribution of
random normal variable $\beta_0$. However, note that the mode of the
baseline distribution (9.043) is slightly to the right of the true
distribution (9.0). 


```{r beta1, echo=FALSE, results='asis', strip.white=TRUE}
plot(density(bhat.Baseline$b1hat),
     col="midnightblue", xlab="", ylab="Density", lwd=2, 
     main=expression(paste("Distribution of ", beta[1], sep="  ")))
lines(x=b1True[, 1], y=b1True[, 2], lwd=2, col="firebrick")
legend("topright", col=c("midnightblue", "firebrick"),
       lwd=c(2,2), bty="n", legend=c("Monte-carlo distribution",
                                     "True distribution"))
```

The figure above shows the distribution of $\hat \beta_1$ in the
baseline model compared to the *true* distribution of $\beta_1$. The
baseline distribution looks like the random normal distribution albeit
with some deviations. The mode in this case is to the left of the mode
of the true distribution.

Note that, over repeated sampling (for, very large $N$), on average the distribution for the
estimates will match asymptotically to the true distribution. (That
is, the left and right skewed distributions will average out.)

# Question 2
*Now vary the sample size ($n$) and redo 1) under the new
values. Report and plot your results. Comment.*

Increasing $n$ implies that the researcher has more information about
the relation between $y$ and $X$. Thus, if you have more data for
estimation, the variance of the estimates will be lower. In addition,
the estimates of regression coefficients will also be closer to the
true value. To see the effect of change in sample size ($n$), I run
the model with three different sample size:

1. $n=10$ (Small sample)
2. $n=100$ (Baseline)
3. $n=1000$ (Large sample)

Mathematically:

$var(\hat \beta | X) = \sigma_{\epsilon}^2 (X'X)^{-1}$
Large sample size implies a higher value for $(X'X)^{-1}$ and a lower
value for $var(\hat \beta | X)$.

The table given below reports the expected value of $\hat \beta$
conditional on X.

| Sample size ($n$) | $E[\hat \beta_0|X]$ | $E[\hat \beta_1|X]$ |
|-------------------|--------------------:|--------------------:|
| n=10              |               8.826 |               2.019 |
| n=100             |               9.043 |               1.997 |
| n=1000            |               8.988 |               2.001 |

The table given below reports the expected variance of $\hat \beta$
conditional on X.

| Sample size ($n$) | $v[\hat \beta_0|X]$ | $v[\hat \beta_1|X]$ |
|-------------------|--------------------:|--------------------:|
| $n=10$            |              34.155 |               0.336 |
| $n=100$           |               0.267 |               0.003 |
| $n=1000$          |               0.003 |              0.0002 |


```{r q2b0hat, echo = FALSE, results = 'asis', strip.white=TRUE}
ggplot(bhat.n, aes(x=b0hat, fill=type)) +
    geom_density(alpha=0.5) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[0], sep="   "))) +
    theme(legend.background=element_rect(),
          legend.position=c(.8,.85),
          plot.title=element_text(hjust=.5))  +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name = "",
                      breaks=c("Small Sample",
                               "Baseline Sample", "Large Sample"),
                      labels=c("Sample size - 10", "Sample size - 100",
                               "Sample size - 1000"))

```

The figure above shows the distribution of $\hat \beta_0$ for
different values of $n$. As we increase the sample size from 10 to
1000, we see that the expected value of $\hat \beta_0$ reaches close
to the true value of $\beta_0$. In addition, the variance of the
distribution reduces as we move from $n=10$ to $n=1000$.

```{r q2b1hat, echo=FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.n, aes(x=b1hat, fill=type)) +
    geom_density(alpha=0.5) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[1], sep="  "))) +
    theme(legend.background=element_rect(),
          legend.position=c(.8,.85),
          plot.title=element_text(hjust=.5))  +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name = "",
                      breaks=c("Small Sample",
                               "Baseline Sample", "Large Sample"),
                      labels=c("Sample size - 10", "Sample size - 100",
                               "Sample size - 1000"))
```

The figure shows the distribution of $\hat \beta_1$ as we increase the
size of our sample. Once again, the expected value of $\hat \beta_1$
reaches close to the true value of $\beta_1$ for large $n$ (and, the
variance is lower).

# Question 3
*Now vary the number of samples ($N$) and redo 1) under the new value.s
Report and plot your results. Comment.*

Increasing the sample size, one would expect the expected values of
coefficients to be closer to the true value and vice
versa. Intuitively, one can think of drawing repeated samples from the
same distribution. If we draw $N$ repeated samples (where, $N$ is
large), on average one should be able to estimate coefficients that
are closer to the true value. In order to see this effect on the
estimates, I vary the sample size, $N$, using three different 
criterion:

1. N=10
2. N=100
3. N=10000

The table given below shows the expected values of $\hat \beta$
conditional on X.

| No. of samples ($N$) | $E[\hat \beta_0|X]$ | $E[\hat \beta_1|X]$ |
|----------------------|--------------------:|--------------------:|
| N=10                 |               9.025 |               1.998 |
| N=100                |               9.043 |               1.997 |
| N=10000              |               9.001 |               2.000 |


```{r q3b0hat, echo = FALSE, results = 'asis', strip.white = TRUE}
ggplot(bhat.N, aes(x=b0hat, fill=type)) +
    geom_density(alpha=0.5) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[0]))) +
    theme(legend.background=element_rect(),
          legend.position=c(.77,.85),
          plot.title=element_text(hjust=.5))  +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name = "",
                      breaks=c("Small Sample", "Baseline", "Large Sample"),
                      labels=c("No. of samples - 10",
                               "No. of samples - 100 (Baseline Model)",
                               "No. of samples - 10000"))

```

```{r q3b1hat, echo=FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.N, aes(x=b1hat, fill=type)) +
    geom_density(alpha=0.5) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[1], sep="   "))) +
    theme(legend.background=element_rect(),
          legend.position=c(.77,.85),
          plot.title=element_text(hjust=.5))  +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name = "",
                      breaks=c("Small Sample", "Baseline", "Large Sample"),
                      labels=c("No. of samples - 10",
                               "No. of samples - 100 (Baseline Model)",
                               "No. of samples - 10000"))
```

# Question 4
*Now vary the distribution of X and redo 1) under the new
assumptions. Report and plot your results. Comment.*

We vary the distribution of $X$ by changing the variance of $X$. Note
that, we derived in class that the variance of slope coefficients is
inversly proportional to $(X'X)$. In particular,

$var(\hat \beta | X) = \sigma_{\epsilon}^2 (X'X)^{-1}$

Thus, if we increase (decrease) the variation in $X$, one would expect that the
variation in $\hat \beta$ will decrease (increase).

| Dist. of X      | $E[\hat \beta_0|X]$ | $E[\hat \beta_1|X]$ |
|-----------------|--------------------:|--------------------:|
| $\sigma_X^2=2$  |               9.054 |               1.996 |
| $\sigma_X^2=4$  |               9.043 |               1.997 |
| $\sigma_X^2=16$ |               9.029 |               1.999 |


```{r q4b0hat, echo=FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.Xvar, aes(x=b0hat, fill=type)) +
    geom_density(alpha=0.3) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[0]))) +
    theme(legend.background=element_rect(),
          legend.position=c(.9,.85),
          plot.title=element_text(hjust=.5)) +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name="",
                      breaks=c("Low", "Baseline", "High"),
                      labels=c(expression(paste(sigma[x]^2,"=", 2)),
                               expression(paste(sigma[x]^2,"=",4)),
                               expression(paste(sigma[x]^2,"=", 16))))

```

The figure shows the variation in $\hat \beta_0$ for three cases a)
$\sigma_X^2=2$, b) $\sigma_X^2=4$ (baseline model), and c)
$\sigma_X^2=16$. In case *a)*, $X$ has lower variance, which implies
that the variation in $\hat \beta_0$ will be higher and thus, the
distribution will be wider. Intuitively, lower variance in X implies
that we have concentration of data in a smaller range. It becomes difficult
to find the true $\hat \beta$ since we can not know how the
relationship holds outside the range. Similarly, in case *c)*, $X$ has
higher variance and hence, the distribution of $\hat \beta_0$ will
shrink compared to the baseline model in *b)*.

        
```{r q4b1hat, echo=FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.Xvar, aes(x=b1hat, fill=type)) +
    geom_density(alpha=0.3) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[1]))) +
    theme(legend.background=element_rect(),
          legend.position=c(.9,.85),
          plot.title=element_text(hjust=.5)) +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name="",
                      breaks=c("Low", "Baseline", "High"),
                      labels=c(expression(paste(sigma[x]^2,"=", 2)),
                               expression(paste(sigma[x]^2,"=",4)),
                               expression(paste(sigma[x]^2,"=", 16))))
```
The figure above shows the variation in $\hat \beta_1$ for the cases
mentioned above. The results are similar to $\hat \beta_0$ where the
variance of estimates decreases as the variation in $X$ increases.

# Question 5
*Now set the mean of the error $\epsilon$ to be different from zero and
redo 1) under the new values. Comment.*

In particular, one can, mathematically, show that a change in the mean of $\epsilon$
affects *only* the intercept. Consider $E[\mu_{\epsilon}] = a$ (some constant,
say). One can re-write the original model as:

$y_i = \tilde{\beta_0} + x_i \beta_1 + u_i$

where, $\tilde{\beta_0}=\beta_0+a$, and, $u_i = \epsilon_i-a$.
Thus, we get, $E[\mu_{u_i}]=0$
Therefore, one would expect the intercept to shift by $a$ and the
slope coefficient to remain unchanged.

| Error term         | $E[\hat \beta_0|X]$ | $E[\hat \beta_1|X]$ |
|--------------------|--------------------:|--------------------:|
| $\mu_{\epsilon}=2$ |              11.043 |               1.997 |
| $\mu_{\epsilon}=0$ |               9.043 |               1.997 |


```{r q5b0hat, echo=FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.eps, aes(x=b0hat, fill=type)) +
    geom_density(alpha=0.3) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[0]))) +
    theme(legend.background=element_rect(),
          legend.position=c(.9,.9),
          plot.title=element_text(hjust=.5)) +
    scale_fill_manual(values=c("firebrick", "midnightblue"),
                      name="",
                      breaks=c("Mean", "No mean"),
                      labels=c(expression(paste(mu[epsilon],"=", 2)),
                               expression(paste(mu[epsilon],"=",0))))
```

The figure above compares the estimate for $\hat \beta_0$ for two
cases, namely: a) $\mu_{\epsilon}=2$ and b) $\mu_{\epsilon}=0$. As
explained above, we see that case *a)* with non-zero (positive) mean for the
error shifts the distribution of $\hat \beta_0$ to the right when
compared to case *b)* (Baseline model)


```{r q5b1hat, echo=FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.eps, aes(x=b1hat, fill=type)) +
    geom_density(alpha=0.3) +
    theme_bw() +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[1]))) +
    theme(legend.background=element_rect(),
          legend.position=c(.9,.9),
          plot.title=element_text(hjust=.5)) +
    scale_fill_manual(values=c("firebrick", "midnightblue"),
                      name="",
                      breaks=c("Mean", "No mean"),
                      labels=c(expression(paste(mu[epsilon],"=", 2)),
                               expression(paste(mu[epsilon],"=",0))))

```

The figure compares the distribution of $\hat \beta_1$ for the cases
mentioned above. As expected, the slope coefficients do not change,
and the two distributions superimpose on each other.

# Question 6
*Now draw the $\epsilon$ in such a way that it is in fact correlated
with $X$ and redo 1) under the new assumptions. Report and plot your
results. Comment.*


| Correlation coeff      | $E[\hat \beta_0|X]$ | $E[\hat \beta_1|X]$ |
|------------------------|--------------------:|--------------------:|
| $r_{X, \epsilon}=0.8$  |               7.040 |               2.197 |
| $r_{X, \epsilon}=0$    |               9.043 |               1.997 |
| $r_{X, \epsilon}=-0.8$ |              11.036 |               1.798 |

```{r q6b0hat, echo = FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.cor, aes(x=b0hat, fill=type)) +
    geom_density(alpha=0.3) +
    theme_bw() +
    theme(legend.background=element_rect(),
          legend.position=c(.9,.85),
          plot.title=element_text(hjust=.5)) +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[0]))) +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name="",
                      breaks=c("Correlated", "Baseline", "Uncorrelated"),
                      labels=c(expression(paste(r[x][epsilon],"=",0.8)),
                               expression(paste(r[x][epsilon],"=",0)),
                               expression(paste(r[x][epsilon],"=",-0.8))))                              
```

The figure above compares the distribution of $\hat \beta_0$ for three
cases: namely a) $r_{X,\epsilon} = 0.8$, b) $r_{X, \epsilon} = 0$, and c)
$r_{X,\epsilon}=-0.8$. When $X$ and $\epsilon$ are positively (or, negatively)
correlated, the strict exogeneity assumption (A1.2) is violated. This
leads to OLS estimates being biased. In particular, positive
(negative) correlation leads to a downward (upward) bias in our
estimates.

```{r q6b1hat, echo=FALSE, results='asis', strip.white=TRUE}
ggplot(bhat.cor, aes(x=b1hat, fill=type)) +
    geom_density(alpha=0.3) +
    theme_bw() +
    theme(legend.background=element_rect(),
          legend.position=c(.9,.85),
          plot.title=element_text(hjust=.5)) +
    labs(x="", y="Density", title=expression(paste("Distribution of ", beta[1]))) +
    scale_fill_manual(values=c("firebrick", "midnightblue", "forestgreen"),
                      name="",
                      breaks=c("Correlated", "Baseline", "Uncorrelated"),
                      labels=c(expression(paste(r[x][epsilon],"=",0.8)),
                               expression(paste(r[x][epsilon],"=",0)),
                               expression(paste(r[x][epsilon],"=",-0.8))))
```

The figure shown above compares the distribution of $\hat \beta_1$ for
the three cases as mentioned above. Once again, correlation between
the regressor and error terms leads to a violation of strict
exogeneity assumption resulting in biased estimates of $\hat
\beta_1$. However, note that in this case, positive (negative)
correlation leads to an upward (downward) bias for $\hat \beta_1$.
